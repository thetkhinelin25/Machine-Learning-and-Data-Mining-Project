{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae211df",
   "metadata": {},
   "source": [
    "# ANN Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053020a2",
   "metadata": {},
   "source": [
    "This notebook builds and trains two Artificial Neural Network (ANN) using `Updated_Metadata_2.csv` dataset. The models are developed using the **same architecture and hyperparameters** but one use EarlyStopping and the other one doesn't.\n",
    "\n",
    "### Key Steps:\n",
    "\n",
    "1. **Data Preparation**\n",
    "   - Loads training and testing datasets.\n",
    "   - Label encodes the target variable and applies one-hot encoding for neural network compatibility.\n",
    "   - Features are standardized using `StandardScaler`.\n",
    "\n",
    "2. **Model Architecture**\n",
    "   - A deep neural network is defined with 4 hidden layers (500 → 300 → 100 → 45 neurons).\n",
    "   - Each layer uses ReLU activation, batch normalization, and dropout regularization.\n",
    "   - The output layer uses softmax activation for multi-class classification.\n",
    "\n",
    "3. **Model Training**\n",
    "   - The model is compiled with Adam optimizer and categorical cross-entropy loss.\n",
    "   - **EarlyStopping** is applied to prevent overfitting in one version of the model.\n",
    "   - Two versions are trained:\n",
    "     - **Kaggle-optimized ANN model**: trained without EarlyStopping and tuned for highest Kaggle accuracy.\n",
    "     - **Generalized ANN model**: uses EarlyStopping for stability and better performance on test data.\n",
    "     \n",
    "4. **Evaluation and Saving**\n",
    "   - Both models are evaluated using test accuracy, balanced accuracy and Classification Report.\n",
    "   - The models are saved as `ann_best_kaggle.h5` and `ann_best_generalized.h5`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d1b91d",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19b0a6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.initializers import GlorotUniform, HeNormal\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, balanced_accuracy_score, average_precision_score\n",
    "\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64bb9bb",
   "metadata": {},
   "source": [
    "## ANN for Kaggle Competation Only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2d618e",
   "metadata": {},
   "source": [
    "### Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d3702da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "training_df = pd.read_csv(\"Updated_Metadata_2.csv\")\n",
    "testing_df = pd.read_csv(\"Updated_Metadata_Test_2.csv\")\n",
    "\n",
    "# Encode the target variable\n",
    "target_column = \"activity\"\n",
    "label_encoder = LabelEncoder()\n",
    "training_df[target_column] = label_encoder.fit_transform(training_df[target_column])\n",
    "testing_df[target_column] = label_encoder.fit_transform(testing_df[target_column])\n",
    "\n",
    "# Separate features and target\n",
    "X_train = training_df.drop(columns=[\"user_snippet\", target_column])  # Exclude non-numeric and target columns\n",
    "y_train = training_df[target_column]\n",
    "\n",
    "X_test = testing_df.drop(columns=[\"user_snippet\", target_column])  # Exclude non-numeric and target columns\n",
    "y_test = testing_df[target_column]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf90329",
   "metadata": {},
   "source": [
    "### Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c548833",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "169/169 [==============================] - 2s 4ms/step - loss: 0.5619 - accuracy: 0.8025 - val_loss: 1.2959 - val_accuracy: 0.7837\n",
      "Epoch 2/12\n",
      "169/169 [==============================] - 1s 4ms/step - loss: 0.2851 - accuracy: 0.9051 - val_loss: 1.5587 - val_accuracy: 0.7702\n",
      "Epoch 3/12\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 0.2446 - accuracy: 0.9156 - val_loss: 1.3244 - val_accuracy: 0.7821\n",
      "Epoch 4/12\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 0.2069 - accuracy: 0.9328 - val_loss: 1.9158 - val_accuracy: 0.7829\n",
      "Epoch 5/12\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 0.1909 - accuracy: 0.9340 - val_loss: 1.6908 - val_accuracy: 0.7845\n",
      "Epoch 6/12\n",
      "169/169 [==============================] - 1s 4ms/step - loss: 0.1745 - accuracy: 0.9382 - val_loss: 1.6272 - val_accuracy: 0.7924\n",
      "Epoch 7/12\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 0.1676 - accuracy: 0.9431 - val_loss: 1.6852 - val_accuracy: 0.7734\n",
      "Epoch 8/12\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 0.1569 - accuracy: 0.9483 - val_loss: 1.7728 - val_accuracy: 0.7655\n",
      "Epoch 9/12\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 0.1458 - accuracy: 0.9513 - val_loss: 1.7651 - val_accuracy: 0.7789\n",
      "Epoch 10/12\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 0.1319 - accuracy: 0.9538 - val_loss: 1.8364 - val_accuracy: 0.7876\n",
      "Epoch 11/12\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 0.1476 - accuracy: 0.9507 - val_loss: 1.9092 - val_accuracy: 0.7892\n",
      "Epoch 12/12\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 0.1186 - accuracy: 0.9608 - val_loss: 1.9128 - val_accuracy: 0.7916\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6865 - accuracy: 0.8252\n",
      "Test Accuracy: 0.8252\n"
     ]
    }
   ],
   "source": [
    "# Get the number of unique classes for output layer\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Convert integer labels to One-Hot Encoding\n",
    "y_train_one_hot = to_categorical(y_train, num_classes)  # Convert to One-Hot Encoding\n",
    "y_test_one_hot = to_categorical(y_test, num_classes)  # Convert to One-Hot Encoding\n",
    "\n",
    "SEED = 999999\n",
    "np.random.seed(SEED)  # Set NumPy seed\n",
    "random.seed(SEED)  # Set Python random seed\n",
    "tf.random.set_seed(SEED)  # Set TensorFlow seed\n",
    "\n",
    "# Define the model with fixed weight initializers\n",
    "model = Sequential([\n",
    "    Dense(500, activation='relu', kernel_initializer=HeNormal(), input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(300, activation='relu', kernel_initializer=HeNormal(), input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(100, activation='relu', kernel_initializer=HeNormal(), input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(45, activation='relu', kernel_initializer=HeNormal(), input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    Dense(num_classes, activation='softmax', kernel_initializer=GlorotUniform())  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "custom_early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    min_delta=0.001\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train_one_hot, \n",
    "                    epochs=12, \n",
    "                    batch_size=30, \n",
    "                    validation_split=0.2, \n",
    "                    verbose=1)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_one_hot)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7d4ae1",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5e520be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as ann.h5\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save(\"ann_best_kaggle.h5\")\n",
    "print(\"Model saved as ann.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0312c183",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b61eac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 0s 942us/step\n",
      "ANN Test Balanced Accuracy: 0.7550009221331074\n",
      "ANN Accuracy: 0.8252326783867632\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Downstairs       0.55      0.59      0.57       174\n",
      "     Jogging       0.94      0.98      0.96       689\n",
      "     Sitting       0.67      1.00      0.80        22\n",
      "    Standing       0.93      0.65      0.77        43\n",
      "    Upstairs       0.64      0.45      0.53       238\n",
      "     Walking       0.82      0.86      0.84       768\n",
      "\n",
      "    accuracy                           0.83      1934\n",
      "   macro avg       0.76      0.76      0.74      1934\n",
      "weighted avg       0.82      0.83      0.82      1934\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_best = load_model(\"ann_best_kaggle.h5\")\n",
    "\n",
    "y_pred_probs_ann = model_best.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred_probs_ann, axis=1)\n",
    "balanced_accuracy_ann = balanced_accuracy_score(y_test, y_pred_classes)\n",
    "accuracy_ann = accuracy_score(y_test, y_pred_classes)\n",
    "report = classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_)\n",
    "\n",
    "print(\"ANN Test Balanced Accuracy:\", balanced_accuracy_ann)\n",
    "print(\"ANN Accuracy:\", accuracy_ann)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbc5f24",
   "metadata": {},
   "source": [
    "### Predict the output for Kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17f62d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model_best = load_model(\"ann_best_kaggle.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50519c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# testing_kaggle_df = pd.read_csv(\"metadata_kaggle2.csv\")\n",
    "testing_kaggle_df = pd.read_csv(\"Updated_Metadata_Kaggle_2.csv\")\n",
    "X_test_ID = testing_kaggle_df[\"user_snippet\"]\n",
    "X_test_kaggle = testing_kaggle_df.drop(columns=[\"user_snippet\"])  # Exclude non-numeric and target columns\n",
    "\n",
    "# Standardize features\n",
    "X_test_kaggle_scaled = scaler.transform(X_test_kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e69d476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 0s 1ms/step\n",
      "Saved the output successfully!\n"
     ]
    }
   ],
   "source": [
    "y_pred_probs_ann = model_best.predict(X_test_kaggle_scaled)\n",
    "y_pred_classes = np.argmax(y_pred_probs_ann, axis=1)\n",
    "y_pred_original = label_encoder.inverse_transform(y_pred_classes)\n",
    "prediction_ann = pd.DataFrame({\"user_snippet\": X_test_ID, \"predicted\" : y_pred_original})\n",
    "prediction_ann.to_csv(\"output_ann_best_kaggle.csv\", index=False)\n",
    "print(\"Saved the output successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b4a55b",
   "metadata": {},
   "source": [
    "## More Generalized ANN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b385ce",
   "metadata": {},
   "source": [
    "### Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76b51210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "training_df = pd.read_csv(\"Updated_Metadata_2.csv\")\n",
    "testing_df = pd.read_csv(\"Updated_Metadata_Test_2.csv\")\n",
    "\n",
    "# Encode the target variable\n",
    "target_column = \"activity\"\n",
    "label_encoder = LabelEncoder()\n",
    "training_df[target_column] = label_encoder.fit_transform(training_df[target_column])\n",
    "testing_df[target_column] = label_encoder.fit_transform(testing_df[target_column])\n",
    "\n",
    "# Separate features and target\n",
    "X_train = training_df.drop(columns=[\"user_snippet\", target_column])  # Exclude non-numeric and target columns\n",
    "y_train = training_df[target_column]\n",
    "\n",
    "X_test = testing_df.drop(columns=[\"user_snippet\", target_column])  # Exclude non-numeric and target columns\n",
    "y_test = testing_df[target_column]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee20967",
   "metadata": {},
   "source": [
    "### Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50d15003",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "169/169 [==============================] - 2s 4ms/step - loss: 0.5619 - accuracy: 0.8025 - val_loss: 1.2959 - val_accuracy: 0.7837\n",
      "Epoch 2/100\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 0.2851 - accuracy: 0.9051 - val_loss: 1.5587 - val_accuracy: 0.7702\n",
      "Epoch 3/100\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 0.2446 - accuracy: 0.9156 - val_loss: 1.3244 - val_accuracy: 0.7821\n",
      "Epoch 4/100\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 0.2069 - accuracy: 0.9328 - val_loss: 1.9158 - val_accuracy: 0.7829\n",
      "Epoch 5/100\n",
      "169/169 [==============================] - 0s 3ms/step - loss: 0.1909 - accuracy: 0.9340 - val_loss: 1.6908 - val_accuracy: 0.7845\n",
      "Epoch 6/100\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 0.1745 - accuracy: 0.9382 - val_loss: 1.6272 - val_accuracy: 0.7924\n",
      "Epoch 7/100\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 0.1676 - accuracy: 0.9431 - val_loss: 1.6852 - val_accuracy: 0.7734\n",
      "Epoch 8/100\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 0.1569 - accuracy: 0.9483 - val_loss: 1.7728 - val_accuracy: 0.7655\n",
      "Epoch 9/100\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 0.1458 - accuracy: 0.9513 - val_loss: 1.7651 - val_accuracy: 0.7789\n",
      "Epoch 10/100\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 0.1319 - accuracy: 0.9538 - val_loss: 1.8364 - val_accuracy: 0.7876\n",
      "Epoch 11/100\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 0.1476 - accuracy: 0.9507 - val_loss: 1.9092 - val_accuracy: 0.7892\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6379 - accuracy: 0.8532\n",
      "Test Accuracy: 0.8532\n"
     ]
    }
   ],
   "source": [
    "# Get the number of unique classes for output layer\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Convert integer labels to One-Hot Encoding\n",
    "y_train_one_hot = to_categorical(y_train, num_classes)  # Convert to One-Hot Encoding\n",
    "y_test_one_hot = to_categorical(y_test, num_classes)  # Convert to One-Hot Encoding\n",
    "\n",
    "SEED = 999999\n",
    "np.random.seed(SEED)  # Set NumPy seed\n",
    "random.seed(SEED)  # Set Python random seed\n",
    "tf.random.set_seed(SEED)  # Set TensorFlow seed\n",
    "\n",
    "# Define the model with fixed weight initializers\n",
    "model = Sequential([\n",
    "    Dense(500, activation='relu', kernel_initializer=HeNormal(), input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(300, activation='relu', kernel_initializer=HeNormal(), input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(100, activation='relu', kernel_initializer=HeNormal(), input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(45, activation='relu', kernel_initializer=HeNormal(), input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    Dense(num_classes, activation='softmax', kernel_initializer=GlorotUniform())  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "custom_early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    min_delta=0.001\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train_one_hot, \n",
    "                    epochs=100, \n",
    "                    batch_size=30, \n",
    "                    validation_split=0.2, \n",
    "                    verbose=1, \n",
    "                    callbacks=[custom_early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_one_hot)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6348798e",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6067761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as ann.h5\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save(\"ann_best_generalized.h5\")\n",
    "print(\"Model saved as ann.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a9397",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab035f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 0s 955us/step\n",
      "ANN Test Balanced Accuracy: 0.8076698930227194\n",
      "ANN Accuracy: 0.8531540847983454\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Downstairs       0.57      0.70      0.63       174\n",
      "     Jogging       0.93      0.98      0.96       689\n",
      "     Sitting       0.67      1.00      0.80        22\n",
      "    Standing       0.94      0.70      0.80        43\n",
      "    Upstairs       0.71      0.62      0.67       238\n",
      "     Walking       0.90      0.85      0.88       768\n",
      "\n",
      "    accuracy                           0.85      1934\n",
      "   macro avg       0.79      0.81      0.79      1934\n",
      "weighted avg       0.86      0.85      0.85      1934\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_best = load_model(\"ann_best_generalized.h5\")\n",
    "\n",
    "y_pred_probs_ann = model_best.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred_probs_ann, axis=1)\n",
    "balanced_accuracy_ann = balanced_accuracy_score(y_test, y_pred_classes)\n",
    "accuracy_ann = accuracy_score(y_test, y_pred_classes)\n",
    "report = classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_)\n",
    "\n",
    "print(\"ANN Test Balanced Accuracy:\", balanced_accuracy_ann)\n",
    "print(\"ANN Accuracy:\", accuracy_ann)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704457a9",
   "metadata": {},
   "source": [
    "### Predict the output for Kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c180fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model_best = load_model(\"ann_best_generalized.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9bb62c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# testing_kaggle_df = pd.read_csv(\"metadata_kaggle2.csv\")\n",
    "testing_kaggle_df = pd.read_csv(\"Updated_Metadata_Kaggle_2.csv\")\n",
    "X_test_ID = testing_kaggle_df[\"user_snippet\"]\n",
    "X_test_kaggle = testing_kaggle_df.drop(columns=[\"user_snippet\"])  # Exclude non-numeric and target columns\n",
    "\n",
    "# Standardize features\n",
    "X_test_kaggle_scaled = scaler.transform(X_test_kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a9b096c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 0s 915us/step\n",
      "Saved the output successfully!\n"
     ]
    }
   ],
   "source": [
    "y_pred_probs_ann = model_best.predict(X_test_kaggle_scaled)\n",
    "y_pred_classes = np.argmax(y_pred_probs_ann, axis=1)\n",
    "y_pred_original = label_encoder.inverse_transform(y_pred_classes)\n",
    "prediction_ann = pd.DataFrame({\"user_snippet\": X_test_ID, \"predicted\" : y_pred_original})\n",
    "prediction_ann.to_csv(\"output_ann_best_generalized.csv\", index=False)\n",
    "print(\"Saved the output successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b86bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
